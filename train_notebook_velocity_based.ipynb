{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "from diffusers.schedulers.scheduling_ddpm import DDPMScheduler\n",
    "from diffusers.training_utils import EMAModel\n",
    "from diffusers.optimization import get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "import submodules.data_filter as _df\n",
    "import diffusion_pipline.data_processing as dproc\n",
    "import diffusion_pipline.model as md\n",
    "import submodules.cleaned_file_parser as cfp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 6.651803e+07\n"
     ]
    }
   ],
   "source": [
    "# observation and action dimensions corrsponding to\n",
    "# the output of PushTEnv\n",
    "# obs_dim = 25\n",
    "# action_dim = 13\n",
    "\n",
    "obs_dim = 45\n",
    "action_dim = 12\n",
    "# parameters\n",
    "pred_horizon = 16\n",
    "obs_horizon = 2\n",
    "action_horizon = 8\n",
    "target_fps = 120.0\n",
    "\n",
    "action_item = ['chisel', 'gripper']\n",
    "obs_item = ['battery']\n",
    "\n",
    "# create network object\n",
    "noise_pred_net = md.ConditionalUnet1D(\n",
    "    input_dim=action_dim,\n",
    "    global_cond_dim=obs_dim*obs_horizon\n",
    ")\n",
    "\n",
    "# example inputs\n",
    "noised_action = torch.randn((1, pred_horizon, action_dim))\n",
    "obs = torch.zeros((1, obs_horizon, obs_dim))\n",
    "diffusion_iter = torch.zeros((1,))\n",
    "\n",
    "# the noise prediction network\n",
    "# takes noisy action, diffusion iteration and observation as input\n",
    "# predicts the noise added to action\n",
    "noise = noise_pred_net(\n",
    "    sample=noised_action,\n",
    "    timestep=diffusion_iter,\n",
    "    global_cond=obs.flatten(start_dim=1))\n",
    "\n",
    "# illustration of removing noise\n",
    "# the actual noise removal is performed by NoiseScheduler\n",
    "# and is dependent on the diffusion noise schedule\n",
    "denoised_action = noised_action - noise\n",
    "\n",
    "# for this demo, we use DDPMScheduler with 100 diffusion iterations\n",
    "num_diffusion_iters = 100\n",
    "noise_scheduler = DDPMScheduler(\n",
    "    num_train_timesteps=num_diffusion_iters,\n",
    "    # the choise of beta schedule has big impact on performance\n",
    "    # we found squared cosine works the best\n",
    "    beta_schedule='squaredcos_cap_v2',\n",
    "    # clip output to [-1,1] to improve stability\n",
    "    clip_sample=True,\n",
    "    # our network predicts noise (instead of denoised action)\n",
    "    prediction_type='epsilon'\n",
    ")\n",
    "\n",
    "# device transfer\n",
    "device = torch.device('cuda')\n",
    "_ = noise_pred_net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset from file\n",
    "# path_name = \"/home/cam/Downloads/Supporting Data - Sheet1.csv\"\n",
    "base_path = \"/home/cam/Documents/diffusion_policy_cam/diffusion_pipline/data_chisel_task/cleaned_training_traj/\"\n",
    "\n",
    "# Load data\n",
    "dict_of_df_rigid = {}\n",
    "dict_of_df_marker = {}\n",
    "\n",
    "\n",
    "for file in os.listdir(base_path):\n",
    "    if file.endswith(\".csv\") and file.startswith(\"cap\"):\n",
    "        path_name = base_path + file\n",
    "        data = cfp.DataParser.from_quat_file(file_path = path_name, target_fps=target_fps, filter=False, window_size=15, polyorder=3)\n",
    "        # dict_of_df_rigid[file] = data.get_rigid_TxyzRxyz()\n",
    "        dict_of_df_marker[file] = data.get_marker_Txyz()\n",
    "\n",
    "        data_time = data.get_time().astype(float)\n",
    "        data_state_dict = data.get_rigid_TxyzRxyz()\n",
    "\n",
    "        # use the time and state data to get the velocity data\n",
    "        data_velocity_dict = {}\n",
    "        for key in data_state_dict.keys():\n",
    "            data_velocity_dict[key] = np.zeros_like(data_state_dict[key])\n",
    "            for i in range(1, len(data_time)):\n",
    "                data_velocity_dict[key][i] = (data_state_dict[key][i] - data_state_dict[key][i-1]) / (data_time[i] - data_time[i-1])\n",
    "                velocity_data = pd.DataFrame(data_velocity_dict[key], columns = [f'{key}_X', f'{key}_Y', f'{key}_Z', f'{key}_x', f'{key}_y', f'{key}_z'])\n",
    "                filtered_velocity = _df.apply_savgol_filter(velocity_data, window_size = 15, polyorder = 3, time_frame= False)\n",
    "                data_velocity_dict[key] = filtered_velocity.values\n",
    "\n",
    "        dict_of_df_rigid[file] = data_velocity_dict\n",
    "\n",
    "item_name = data.rigid_bodies\n",
    "marker_name = data.markers\n",
    "\n",
    "if len(dict_of_df_rigid) == len(dict_of_df_marker):\n",
    "\n",
    "    rigiddataset, index = _df.episode_combiner(dict_of_df_rigid, item_name)\n",
    "    markerdataset, _ = _df.episode_combiner(dict_of_df_marker, marker_name)\n",
    "    print(index[action_item[0]])\n",
    "\n",
    "\n",
    "#### if you don't want battery info then just do obs_item = None abd also do clear all outputs and restart the kernal before that and satrt from the top \n",
    "\n",
    "dataset = dproc.TaskStateDataset(rigiddataset, markerdataset, index[action_item[0]], \n",
    "                                 action_item = action_item, obs_item = obs_item,\n",
    "                                 marker_item= marker_name,\n",
    "                                 pred_horizon=pred_horizon,\n",
    "                                 obs_horizon=obs_horizon,\n",
    "                                 action_horizon=action_horizon)\n",
    "\n",
    "# create dataloader\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=256,\n",
    "    num_workers=1,\n",
    "    shuffle=True,\n",
    "    # accelerate cpu-gpu transfer\n",
    "    pin_memory=True,\n",
    "    # don't kill worker process afte each epoch\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "batch = next(iter(dataloader))\n",
    "print(\"batch['obs'].shape:\", batch['obs'].shape)\n",
    "print(\"batch['action'].shape\", batch['action'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown ### **Training**\n",
    "#@markdown\n",
    "#@markdown Takes about an hour. If you don't want to wait, skip to the next cell\n",
    "#@markdown to load pre-trained weights\n",
    "\n",
    "num_epochs =200\n",
    "checkpoint_dir = 'checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "checkpoint_interval = 3600\n",
    "last_checkpoint_time = time.time()\n",
    "# Exponential Moving Average\n",
    "# accelerates training and improves stability\n",
    "# holds a copy of the model weights\n",
    "ema = EMAModel(\n",
    "    parameters=noise_pred_net.parameters(),\n",
    "    power=0.75)\n",
    "\n",
    "# Standard ADAM optimizer\n",
    "# Note that EMA parametesr are not optimized\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params=noise_pred_net.parameters(),\n",
    "    lr=1e-4, weight_decay=1e-6)\n",
    "\n",
    "# Cosine LR schedule with linear warmup\n",
    "lr_scheduler = get_scheduler(\n",
    "    name='cosine',\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=200,\n",
    "    num_training_steps=len(dataloader) * num_epochs\n",
    ")\n",
    "\n",
    "with tqdm(range(num_epochs), desc='Epoch') as tglobal:\n",
    "    # epoch loop\n",
    "    epoch_loss = []\n",
    "    batch_loss_per_epoch = []\n",
    "\n",
    "    for epoch_idx in tglobal:\n",
    "        batch_loss = []\n",
    "        batch_noise = []\n",
    "        # batch loop\n",
    "        with tqdm(dataloader, desc='Batch', leave=False) as tepoch:\n",
    "\n",
    "            for nbatch in tepoch:\n",
    "                # data normalized in dataset\n",
    "                # device transfer\n",
    "                nobs = nbatch['obs']\n",
    "                naction = nbatch['action']\n",
    "                B = nobs.shape[0]\n",
    "\n",
    "                # observation as FiLM conditioning\n",
    "                # (B, obs_horizon, obs_dim)\n",
    "                obs_cond = nobs[:,:obs_horizon,:]\n",
    "                # (B, obs_horizon * obs_dim)\n",
    "                obs_cond = obs_cond.flatten(start_dim=1).float().to(device)\n",
    "                # print(obs_cond.type())\n",
    "\n",
    "                # sample noise to add to actions\n",
    "                # noise = torch.randn(naction.shape, device=device)\n",
    "                noise = torch.randn(naction.shape)\n",
    "\n",
    "                # sample a diffusion iteration for each data point\n",
    "                timesteps = torch.randint(\n",
    "                    0, noise_scheduler.config.num_train_timesteps,\n",
    "                    (B,)\n",
    "                ).long()\n",
    "\n",
    "                # add noise to the clean images according to the noise magnitude at each diffusion iteration\n",
    "                # (this is the forward diffusion process)\n",
    "                noisy_actions = noise_scheduler.add_noise(\n",
    "                    naction, noise, timesteps)\n",
    "                \n",
    "                noise = noise.to(device)\n",
    "                \n",
    "                timesteps = timesteps.to(device)\n",
    "\n",
    "                # print(noisy_actions.type())\n",
    "                noisy_actions = noisy_actions.type(torch.FloatTensor).to(device)\n",
    "                # print(noisy_actions.type())\n",
    "\n",
    "                # predict the noise residual\n",
    "                noise_pred = noise_pred_net(\n",
    "                    noisy_actions, timesteps, global_cond=obs_cond)\n",
    "                \n",
    "                batch_noise.append(noise_pred)\n",
    "\n",
    "                # L2 loss\n",
    "                loss = nn.functional.mse_loss(noise_pred, noise)\n",
    "\n",
    "                # optimize\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                # step lr scheduler every batch\n",
    "                # this is different from standard pytorch behavior\n",
    "                lr_scheduler.step()\n",
    "\n",
    "                # update Exponential Moving Average of the model weights\n",
    "                ema.step(noise_pred_net)\n",
    "                # print(ema.state_dict)\n",
    "\n",
    "                # logging\n",
    "                loss_cpu = loss.item()\n",
    "                batch_loss.append(loss_cpu)\n",
    "                tepoch.set_postfix(loss=loss_cpu)\n",
    "\n",
    "        # save checkpoint\n",
    "        # went to the emma model library and added state_dict to the model\n",
    "        current_time = time.time()\n",
    "        if current_time - last_checkpoint_time > checkpoint_interval:\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f'T_checkpoint_epoch_{epoch_idx}.pth')\n",
    "\n",
    "            torch.save({\n",
    "                'epoch': epoch_idx,\n",
    "                'model_state_dict': noise_pred_net.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss.cpu().detach().numpy(),\n",
    "            }, checkpoint_path)\n",
    "            print(f'Checkpoint saved at epoch {epoch_idx}')\n",
    "            last_checkpoint_time = current_time\n",
    "\n",
    "        if epoch_idx == num_epochs:\n",
    "            # Save model checkpoint\n",
    "            # checkpoint_path = os.path.join(checkpoint_dir, f'BOX_GRIP_checkpoint_epoch_{epoch_idx}.pth')\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f'T_checkpoint_epoch_{epoch_idx}.pth')\n",
    "            torch.save({\n",
    "                'epoch': epoch_idx,\n",
    "                'model_state_dict': noise_pred_net.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss.cpu().detach().numpy(),\n",
    "            }, checkpoint_path)\n",
    "            print(f'Checkpoint saved at epoch {epoch_idx}')\n",
    "            last_checkpoint_time = current_time\n",
    "            \n",
    "        tglobal.set_postfix(loss=np.mean(batch_loss))\n",
    "        epoch_loss.append(np.mean(batch_loss))\n",
    "        batch_loss_per_epoch.append(batch_loss)\n",
    "\n",
    "# Weights of the EMA model\n",
    "# is used for inference\n",
    "ema_noise_pred_net = noise_pred_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure and axis\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "flatten_loss = np.array(batch_loss_per_epoch)\n",
    "\n",
    "flatten_loss = flatten_loss.flatten()\n",
    "# Define subplots correctly\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Plot for the first subplot (ax1)\n",
    "ax1.plot(flatten_loss, marker='o', linestyle='-', color='b', label='Values')\n",
    "\n",
    "# Customize the first subplot (ax1)\n",
    "ax1.set_xlabel('Batchs')\n",
    "ax1.set_ylabel('Value')\n",
    "ax1.set_title('Batch Loss for all Epochs')\n",
    "ax1.grid(True)\n",
    "# ax1.legend()\n",
    "\n",
    "# Plot for the second subplot (ax2)\n",
    "ax2.plot(epoch_loss, marker='o', linestyle='-', color='b', label='Values')\n",
    "\n",
    "# Customize the second subplot (ax2)\n",
    "ax2.set_xlabel('Epochs')\n",
    "ax2.set_ylabel('Value')\n",
    "ax2.set_title('Epoch Loss')\n",
    "ax2.grid(True)\n",
    "# ax2.legend()\n",
    "\n",
    "# Adjust layout and display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  CHANGED WAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset from file\n",
    "# path_name = \"/home/cam/Downloads/Supporting Data - Sheet1.csv\"\n",
    "base_path = \"diffusion_pipline/data_chisel_task/test/\"\n",
    "\n",
    "# Load data\n",
    "dict_of_df_rigid_test = {}\n",
    "dict_of_df_marker_test = {}\n",
    "name = []\n",
    "\n",
    "# for file in os.listdir(base_path):\n",
    "#     name.append(file)\n",
    "for file in os.listdir(base_path):\n",
    "    name.append(file)\n",
    "    if file.endswith(\".csv\"):\n",
    "        path_name = base_path + file\n",
    "        data_test = cfp.DataParser.from_quat_file(file_path = path_name, target_fps=target_fps, filter=True, window_size=15, polyorder=3)\n",
    "\n",
    "        dict_of_df_marker_test[file] = data_test.get_marker_Txyz()\n",
    "        data_time = data_test.get_time().astype(float)\n",
    "        data_state_dict = data_test.get_rigid_TxyzRxyz()\n",
    "\n",
    "        # use the time and state data to get the velocity data\n",
    "        data_velocity_dict = {}\n",
    "        for key in data_state_dict.keys():\n",
    "            data_velocity_dict[key] = np.zeros_like(data_state_dict[key])\n",
    "            for i in range(1, len(data_time)):\n",
    "                data_velocity_dict[key][i] = (data_state_dict[key][i] - data_state_dict[key][i-1]) / (data_time[i] - data_time[i-1])\n",
    "                velocity_data = pd.DataFrame(data_velocity_dict[key], columns = [f'{key}_X', f'{key}_Y', f'{key}_Z', f'{key}_x', f'{key}_y', f'{key}_z'])\n",
    "                filtered_velocity = _df.apply_savgol_filter(velocity_data, window_size = 15, polyorder = 3, time_frame= False)\n",
    "                data_velocity_dict[key] = filtered_velocity.values\n",
    "\n",
    "        dict_of_df_rigid_test[file] = data_velocity_dict\n",
    "\n",
    "\n",
    "item_name_test = data_test.rigid_bodies\n",
    "marker_name_test = data_test.markers\n",
    "\n",
    "if len(dict_of_df_rigid_test) == len(dict_of_df_marker_test):\n",
    "\n",
    "    rigiddataset_test, index_test = _df.episode_combiner(dict_of_df_rigid_test, item_name_test)\n",
    "    markerdataset_test, _ = _df.episode_combiner(dict_of_df_marker_test, marker_name_test)\n",
    "\n",
    "index = index_test[action_item[0]]\n",
    "action = []\n",
    "obs = []\n",
    "for i in range(index[-1]):\n",
    "    # a = []\n",
    "    a = np.concatenate([rigiddataset_test[item][i] for item in action_item])\n",
    "    # print(a)\n",
    "\n",
    "    b = np.concatenate([rigiddataset_test[item][i] for item in action_item] + [markerdataset_test[item][i] for item in marker_name_test])\n",
    "    # print(b)\n",
    "    \n",
    "    action.append(a)\n",
    "    obs.append(b)\n",
    "\n",
    "# All demonstration episodes are concatinated in the first dimension N\n",
    "action = np.array(action, dtype=np.float64)\n",
    "obs = np.array(obs, dtype=np.float64)\n",
    "test_data = {\n",
    "    # (N, action_dim)\n",
    "    'action': action[:],\n",
    "    # (N, obs_dim)\n",
    "    'obs': obs[:]\n",
    "}\n",
    "\n",
    "\n",
    "episode_ends = index_test[action_item[0]]\n",
    "\n",
    "splits_obs = []\n",
    "splits_action = []\n",
    "previous_index = 0\n",
    "\n",
    "# Iterate through index_ranges and slice combined_list accordingly\n",
    "for index in episode_ends:\n",
    "    splits_obs.append(test_data['obs'][previous_index:index + 1])  # Include index itself in the slice\n",
    "    splits_action.append(test_data['action'][previous_index:index + 1])\n",
    "    previous_index = index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "trajectories = {}\n",
    "losses_per_traj = {}\n",
    "for j in range(len(episode_ends)):\n",
    "    # print(j)\n",
    "    # get first observation\n",
    "    com_obs = splits_obs[j]\n",
    "    obs = splits_obs[j][0]\n",
    "    actions_test = splits_action[j]\n",
    "    # max_steps = len(test_data['action'])\n",
    "    max_steps = len(actions_test)\n",
    "    stats = dataset.stats\n",
    "    # keep a queue of last 2 steps of observations\n",
    "    obs_deque = collections.deque(\n",
    "        [obs] * obs_horizon, maxlen=obs_horizon)\n",
    "\n",
    "    # save visualization and rewards\n",
    "    done = False\n",
    "    step_idx = 0\n",
    "    traj = []\n",
    "    loss_com = []\n",
    "    with tqdm(total=max_steps, desc=\"Eval\") as pbar:\n",
    "        while not done:\n",
    "            B = 1\n",
    "            # stack the last obs_horizon (2) number of observations\n",
    "            obs_seq = np.stack(obs_deque)\n",
    "            # print(\"Obs_sep -\",obs_seq)\n",
    "            # normalize observation\n",
    "            nobs = dproc.normalize_data(obs_seq, stats=stats['obs'])\n",
    "            # print(nobs)\n",
    "            # device transfer\n",
    "            nobs = torch.from_numpy(nobs).to(device, dtype=torch.float32)\n",
    "            # infer action\n",
    "            with torch.no_grad():\n",
    "                # reshape observation to (B,obs_horizon*obs_dim)\n",
    "                obs_cond = nobs.unsqueeze(0).flatten(start_dim=1)\n",
    "                # print(obs_cond.shape)\n",
    "\n",
    "                # initialize action from Guassian noise\n",
    "                noisy_action = torch.randn(\n",
    "                    (B, pred_horizon, action_dim), device=device)\n",
    "                naction = noisy_action\n",
    "\n",
    "                # init scheduler\n",
    "                noise_scheduler.set_timesteps(num_diffusion_iters)\n",
    "\n",
    "                for k in noise_scheduler.timesteps:\n",
    "                    # predict noise\n",
    "                    noise_pred = ema_noise_pred_net(\n",
    "                        sample=naction,\n",
    "                        timestep=k,\n",
    "                        global_cond=obs_cond\n",
    "                    )\n",
    "\n",
    "                    # inverse diffusion step (remove noise)\n",
    "                    naction = noise_scheduler.step(\n",
    "                        model_output=noise_pred,\n",
    "                        timestep=k,\n",
    "                        sample=naction\n",
    "                    ).prev_sample\n",
    "\n",
    "            # unnormalize action\n",
    "            naction = naction.detach().to('cpu').numpy()\n",
    "            # (B, pred_horizon, action_dim)\n",
    "            # print(len(naction[0]))\n",
    "            naction = naction[0]\n",
    "            action_pred = dproc.unnormalize_data(naction, stats=stats['action'])\n",
    "\n",
    "            # only take action_horizon number of actions\n",
    "            start = obs_horizon - 1\n",
    "            end = start + action_horizon\n",
    "            action = action_pred[start:end,:]\n",
    "            traj.extend(action)\n",
    "            losses = []\n",
    "                \n",
    "            for i in range(len(action)):\n",
    "                # loss\n",
    "            # print(\"Action_pred -\",action[0])\n",
    "            # print(\"Action_orignal -\",actions_test[0])\n",
    "            # print(\"Obs_added without pred-\",com_obs[i])\n",
    "                if len(action) > len(actions_test):\n",
    "                    done = True\n",
    "                if done:\n",
    "                    break\n",
    "                loss_test = nn.functional.mse_loss(torch.tensor(action[i]), torch.tensor(actions_test[i]))\n",
    "                action_last = list(action[i])\n",
    "                # print(\"Action_last ---\",action_last)\n",
    "                com_obs_part = list(com_obs[i][12:])\n",
    "                # print(\"Obs to add\", com_obs_part)\n",
    "                # Concatenating prediction to the obs lists\n",
    "                com_obs[i] = action_last + com_obs_part\n",
    "                # print(\"Obs_added with pred -\",com_obs[i])\n",
    "                obs_deque.append(com_obs[i])\n",
    "                losses.append(loss_test.item())\n",
    "                # update progress bar\n",
    "                step_idx += 1\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix(loss=np.mean(losses))\n",
    "                # print(i)\n",
    "                if step_idx > max_steps:\n",
    "                    done = True\n",
    "                if done:\n",
    "                    break\n",
    "            com_obs = com_obs[len(action):]\n",
    "            actions_test = actions_test[len(action):]\n",
    "            # com_obs = com_obs[1:]\n",
    "            # actions_test = actions_test[1:]\n",
    "            loss_com.append(np.mean(losses).tolist())\n",
    "    losses_per_traj[f\"{name[j]}\"] = np.nanmean(loss_com)\n",
    "    trajectories[f\"{name[j]}\"] = traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_per_traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "with PdfPages('/home/cam/Documents/diffusion_policy_cam/diffusion_pipline/pred_correct_method_velocity/pred_trajectory_chisel_plots.pdf') as pdf:\n",
    "    for index, key in enumerate(trajectories.keys()):\n",
    "\n",
    "        # if index == 1:\n",
    "        #     break\n",
    "\n",
    "        x_coords = [item[3] for item in trajectories[key]]\n",
    "        y_coords = [item[4] for item in trajectories[key]]\n",
    "        z_coords = [item[5] for item in trajectories[key]]\n",
    "        # on_off = [item[12] for item in trajectories[key]]\n",
    "\n",
    "        fig = plt.figure(figsize=(18, 24),constrained_layout=True)\n",
    "        \n",
    "        # Add a global title for the figure\n",
    "        # fig.suptitle(f'Comprehensive Plots for {result_dict[index]['Path'].split('/')[-1]}', fontsize=16, fontweight='bold')\n",
    "        fig.suptitle(f'Comprehensive Plots for {key} - loss = {losses_per_traj[key]}', fontsize=16, fontweight='bold',y=0.998)\n",
    "        # plt.subplots_adjust(top=0.9, hspace=0.4)\n",
    "\n",
    "        # Plot XYZ coordinates\n",
    "        plt.subplot(4, 2, 1)\n",
    "        plt.plot(range(len(x_coords)), x_coords, label='Roll', linestyle='-', linewidth=1)  # Adjust line width here\n",
    "        plt.plot(range(len(y_coords)), y_coords, label='Pitch', linestyle='-', linewidth=1)  # Adjust line width here\n",
    "        plt.plot(range(len(z_coords)), z_coords, label='Yaw', linestyle='-', linewidth=1)  # Adjust line width here\n",
    "        # plt.plot(range(len(on_off)), on_off, label='On-OFF', linestyle='-', linewidth=1)  # Adjust line width here\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.title(f'Pred Gripper- Euler vel')\n",
    "        plt.xlabel('Frame')\n",
    "        plt.ylabel('Velocities')\n",
    "\n",
    "        # Plot Euler angles\n",
    "        roll = [item[0] for item in trajectories[key]]\n",
    "        pitch = [item[1] for item in trajectories[key]]\n",
    "        yaw = [item[2] for item in trajectories[key]]\n",
    "\n",
    "        plt.subplot(4, 2, 2)\n",
    "        plt.plot(range(len(roll)), roll, label='X', linestyle='-', linewidth=1)  # Adjust line width here\n",
    "        plt.plot(range(len(pitch)), pitch, label='Y', linestyle='-', linewidth=1)  # Adjust line width here\n",
    "        plt.plot(range(len(yaw)), yaw, label='Z', linestyle='-', linewidth=1)  # Adjust line width here\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.title(f'Pred Gripper- - XYZ  vel')\n",
    "        plt.xlabel('Frame')\n",
    "        plt.ylabel('Velocities (Radians)')\n",
    "\n",
    "\n",
    "        x_coords = [item[3] for item in splits_action[index]]\n",
    "        y_coords = [item[4] for item in splits_action[index]]\n",
    "        z_coords = [item[5] for item in splits_action[index]]\n",
    "        # on_off = [item[12] for item in splits_action[index]]\n",
    "\n",
    "\n",
    "        # Plot XYZ coordinates\n",
    "        plt.subplot(4, 2, 3)\n",
    "        plt.plot(range(len(x_coords)), x_coords, label='Roll', linestyle='-', linewidth=1)  # Adjust line width here\n",
    "        plt.plot(range(len(y_coords)), y_coords, label='Pitch', linestyle='-', linewidth=1)  # Adjust line width here\n",
    "        plt.plot(range(len(z_coords)), z_coords, label='Yaw', linestyle='-', linewidth=1)  # Adjust line width here\n",
    "        # plt.plot(range(len(on_off)), on_off, label='On-OFF', linestyle='-', linewidth=1)  # Adjust line width here\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.title(f'Orignal Gripper- Euler vel')\n",
    "        plt.xlabel('Frame')\n",
    "        plt.ylabel('Velocities')\n",
    "\n",
    "        # Plot Velocities\n",
    "        roll = [item[0] for item in splits_action[index]]\n",
    "        pitch = [item[1] for item in splits_action[index]]\n",
    "        yaw = [item[2] for item in splits_action[index]]\n",
    "\n",
    "        plt.subplot(4, 2, 4)\n",
    "        plt.plot(range(len(roll)), roll, label='X', linestyle='-', linewidth=1)  # Adjust line width here\n",
    "        plt.plot(range(len(pitch)), pitch, label='Y', linestyle='-', linewidth=1)  # Adjust line width here\n",
    "        plt.plot(range(len(yaw)), yaw, label='Z', linestyle='-', linewidth=1)  # Adjust line width here\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.title(f'Orignal Gripper - XYZ  vel')\n",
    "        plt.xlabel('Frame')\n",
    "        plt.ylabel('Velocities')\n",
    "\n",
    "\n",
    "        x_coords = [item[9] for item in trajectories[key]]\n",
    "        y_coords = [item[10] for item in trajectories[key]]\n",
    "        z_coords = [item[11] for item in trajectories[key]]\n",
    "\n",
    "        # Plot Euler vel\n",
    "        plt.subplot(4, 2, 5)\n",
    "        plt.plot(range(len(x_coords)), x_coords, label='Roll', linestyle='-', linewidth=1)  # Adjust line width here\n",
    "        plt.plot(range(len(y_coords)), y_coords, label='Pitch', linestyle='-', linewidth=1)  # Adjust line width here\n",
    "        plt.plot(range(len(z_coords)), z_coords, label='Yaw', linestyle='-', linewidth=1)  # Adjust line width here\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.title(f'Pred Chisel- Euler vel')\n",
    "        plt.xlabel('Frame')\n",
    "        plt.ylabel('Velocities')\n",
    "\n",
    "        # Plot Velocities\n",
    "        roll = [item[6] for item in trajectories[key]]\n",
    "        pitch = [item[7] for item in trajectories[key]]\n",
    "        yaw = [item[8] for item in trajectories[key]]\n",
    "\n",
    "        plt.subplot(4, 2, 6)\n",
    "        plt.plot(range(len(roll)), roll, label='X', linestyle='-', linewidth=1)  # Adjust line width here\n",
    "        plt.plot(range(len(pitch)), pitch, label='Y', linestyle='-', linewidth=1)  # Adjust line width here\n",
    "        plt.plot(range(len(yaw)), yaw, label='Z', linestyle='-', linewidth=1)  # Adjust line width here\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.title(f'Pred Chisel-  XYZ  vel')\n",
    "        plt.xlabel('Frame')\n",
    "        plt.ylabel('Velocities ')\n",
    "\n",
    "\n",
    "        x_coords = [item[9] for item in splits_action[index]]\n",
    "        y_coords = [item[10] for item in splits_action[index]]\n",
    "        z_coords = [item[11] for item in splits_action[index]]\n",
    "\n",
    "        # Plot Euler vel\n",
    "        plt.subplot(4, 2, 7)\n",
    "        plt.plot(range(len(x_coords)), x_coords, label='Roll', linestyle='-', linewidth=1)  # Adjust line width here\n",
    "        plt.plot(range(len(y_coords)), y_coords, label='Pitch', linestyle='-', linewidth=1)  # Adjust line width here\n",
    "        plt.plot(range(len(z_coords)), z_coords, label='Yaw', linestyle='-', linewidth=1)  # Adjust line width here\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.title(f'Orignal Chisel- Euler vel')\n",
    "        plt.xlabel('Frame')\n",
    "        plt.ylabel('Velocities')\n",
    "\n",
    "        # Plot Velocities\n",
    "        roll = [item[6] for item in splits_action[index]]\n",
    "        pitch = [item[7] for item in splits_action[index]]\n",
    "        yaw = [item[8] for item in splits_action[index]]\n",
    "\n",
    "        plt.subplot(4, 2, 8)\n",
    "        plt.plot(range(len(roll)), roll, label='X', linestyle='-', linewidth=1)  # Adjust line width here\n",
    "        plt.plot(range(len(pitch)), pitch, label='Y', linestyle='-', linewidth=1)  # Adjust line width here\n",
    "        plt.plot(range(len(yaw)), yaw, label='Z', linestyle='-', linewidth=1)  # Adjust line width here\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.title(f'Orignal Chisel-  XYZ  vel')\n",
    "        plt.xlabel('Frame')\n",
    "        plt.ylabel('Velocities')\n",
    "\n",
    "\n",
    "        plt.tight_layout()\n",
    "        pdf.savefig(fig)  # Save the figure with the global title\n",
    "        plt.close(fig)\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "for index, key in enumerate(trajectories.keys()):\n",
    "    print(index)\n",
    "    # Define the file path or name\n",
    "    file_path = f'diffusion_pipline/pred_correct_method_velocity/pred_{key}'\n",
    "\n",
    "    save_type='EULER'\n",
    "    # add first rows\n",
    "    _params = {\n",
    "        'QUAT': {'len':7,\n",
    "                    'dof': ['X', 'Y', 'Z', 'w', 'x', 'y', 'z']},\n",
    "        'EULER': {'len':6,\n",
    "                    'dof': ['X', 'Y', 'Z', 'x', 'y', 'z']}\n",
    "    }\n",
    "    \n",
    "    _SUP_HEADER_ROW = ([\"RigidBody\"] * len(data.rigid_bodies) * _params[save_type]['len'] + [\"Marker\"] * len(data.markers) * 3)\n",
    "    _FPS_ROW = [\"FPS\", target_fps] + [0.0]*(len(_SUP_HEADER_ROW) - 2)\n",
    "    _rb_col_names = [f\"{rb}_{axis}\" for rb in action_item for axis in _params[save_type]['dof']]\n",
    "    _obs_col_name = [f\"{rb}_{axis}\" for rb in obs_item for axis in _params[save_type]['dof']]\n",
    "    _mk_col_names = [f\"{mk}_{axis}\" for mk in marker_name for axis in ['X', 'Y', 'Z']]\n",
    "    _HEADER_ROW = _rb_col_names + _obs_col_name + _mk_col_names\n",
    "    print(len(trajectories[key]))\n",
    "    print(len(splits_obs[index]))\n",
    "    min_length = min(len(trajectories[key]), len(splits_obs[index][11:]))\n",
    "\n",
    "\n",
    "    # Combine up to the minimum length\n",
    "    combined_list = [np.concatenate([x, y[12:]])  for x, y in zip(np.array(trajectories[key][:min_length]), np.array(splits_obs[index][:min_length]))]\n",
    "\n",
    "    \n",
    "    # Open the file in write mode\n",
    "    with open(file_path, 'w') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(_SUP_HEADER_ROW)\n",
    "        writer.writerow(_FPS_ROW)\n",
    "        writer.writerow(_HEADER_ROW)\n",
    "        writer.writerows(combined_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Orignal Way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset from file\n",
    "# path_name = \"/home/cam/Downloads/Supporting Data - Sheet1.csv\"\n",
    "base_path = \"diffusion_pipline/data_chisel_task/test/\"\n",
    "\n",
    "# Load data\n",
    "dict_of_df_rigid_test = {}\n",
    "dict_of_df_marker_test = {}\n",
    "name = []\n",
    "\n",
    "# for file in os.listdir(base_path):\n",
    "#     name.append(file)\n",
    "for file in os.listdir(base_path):\n",
    "    name.append(file)\n",
    "    if file.endswith(\".csv\"):\n",
    "        path_name = base_path + file\n",
    "        data_test = cfp.DataParser.from_quat_file(file_path = path_name, target_fps=target_fps, filter=True, window_size=15, polyorder=3)\n",
    "\n",
    "        dict_of_df_marker_test[file] = data_test.get_marker_Txyz()\n",
    "        data_time = data_test.get_time().astype(float)\n",
    "        data_state_dict = data_test.get_rigid_TxyzRxyz()\n",
    "\n",
    "        # use the time and state data to get the velocity data\n",
    "        data_velocity_dict = {}\n",
    "        for key in data_state_dict.keys():\n",
    "            data_velocity_dict[key] = np.zeros_like(data_state_dict[key])\n",
    "            for i in range(1, len(data_time)):\n",
    "                data_velocity_dict[key][i] = (data_state_dict[key][i] - data_state_dict[key][i-1]) / (data_time[i] - data_time[i-1])\n",
    "                velocity_data = pd.DataFrame(data_velocity_dict[key], columns = [f'{key}_X', f'{key}_Y', f'{key}_Z', f'{key}_x', f'{key}_y', f'{key}_z'])\n",
    "                filtered_velocity = _df.apply_savgol_filter(velocity_data, window_size = 15, polyorder = 3, time_frame= False)\n",
    "                data_velocity_dict[key] = filtered_velocity.values\n",
    "\n",
    "        dict_of_df_rigid_test[file] = data_velocity_dict\n",
    "\n",
    "\n",
    "item_name_test = data_test.rigid_bodies\n",
    "marker_name_test = data_test.markers\n",
    "\n",
    "if len(dict_of_df_rigid_test) == len(dict_of_df_marker_test):\n",
    "\n",
    "    rigiddataset_test, index_test = _df.episode_combiner(dict_of_df_rigid_test, item_name_test)\n",
    "    markerdataset_test, _ = _df.episode_combiner(dict_of_df_marker_test, marker_name_test)\n",
    "\n",
    "index = index_test[action_item[0]]\n",
    "action = []\n",
    "obs = []\n",
    "for i in range(index[-1]):\n",
    "    # a = []\n",
    "    a = np.concatenate([rigiddataset_test[item][i] for item in action_item])\n",
    "    # print(\"Action ----------\",a)\n",
    "\n",
    "    b = np.concatenate([rigiddataset_test[item][i] for item in action_item] + [rigiddataset_test[item][i] for item in obs_item] + [markerdataset_test[item][i] for item in marker_name_test])\n",
    "    # print(\"Obs -------------\",b)\n",
    "    \n",
    "    action.append(a)\n",
    "    obs.append(b)\n",
    "\n",
    "# All demonstration episodes are concatinated in the first dimension N\n",
    "action = np.array(action, dtype=np.float64)\n",
    "obs = np.array(obs, dtype=np.float64)\n",
    "test_data = {\n",
    "    # (N, action_dim)\n",
    "    'action': action[:],\n",
    "    # (N, obs_dim)\n",
    "    'obs': obs[:]\n",
    "}\n",
    "\n",
    "\n",
    "episode_ends = index_test[action_item[0]]\n",
    "\n",
    "splits_obs = []\n",
    "splits_action = []\n",
    "previous_index = 0\n",
    "\n",
    "# Iterate through index_ranges and slice combined_list accordingly\n",
    "for index in episode_ends:\n",
    "    splits_obs.append(test_data['obs'][previous_index:index + 1])  # Include index itself in the slice\n",
    "    splits_action.append(test_data['action'][previous_index:index + 1])\n",
    "    previous_index = index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectories_normal = {}\n",
    "losses_per_traj = {}\n",
    "for j in range(len(episode_ends)):\n",
    "    # print(j)\n",
    "    # get first observation\n",
    "    com_obs = splits_obs[j]\n",
    "    obs = splits_obs[j][0]\n",
    "    actions_test = splits_action[j]\n",
    "    # max_steps = len(test_data['action'])\n",
    "    max_steps = len(actions_test)\n",
    "    stats = dataset.stats\n",
    "    # keep a queue of last 2 steps of observations\n",
    "    obs_deque = collections.deque(\n",
    "        [obs] * obs_horizon, maxlen=obs_horizon)\n",
    "\n",
    "    # save visualization and rewards\n",
    "    done = False\n",
    "    step_idx = 0\n",
    "    traj = []\n",
    "    loss_com = []\n",
    "    with tqdm(total=max_steps, desc=\"Eval\") as pbar:\n",
    "        while not done:\n",
    "            B = 1\n",
    "            # stack the last obs_horizon (2) number of observations\n",
    "            obs_seq = np.stack(obs_deque)\n",
    "            # print(\"Obs_sep -\",obs_seq)\n",
    "            # normalize observation\n",
    "            nobs = dproc.normalize_data(obs_seq, stats=stats['obs'])\n",
    "            # print(nobs)\n",
    "            # device transfer\n",
    "            nobs = torch.from_numpy(nobs).to(device, dtype=torch.float32)\n",
    "            # infer action\n",
    "            with torch.no_grad():\n",
    "                # reshape observation to (B,obs_horizon*obs_dim)\n",
    "                obs_cond = nobs.unsqueeze(0).flatten(start_dim=1)\n",
    "                # print(obs_cond.shape)\n",
    "\n",
    "                # initialize action from Guassian noise\n",
    "                noisy_action = torch.randn(\n",
    "                    (B, pred_horizon, action_dim), device=device)\n",
    "                naction = noisy_action\n",
    "\n",
    "                # init scheduler\n",
    "                noise_scheduler.set_timesteps(num_diffusion_iters)\n",
    "\n",
    "                for k in noise_scheduler.timesteps:\n",
    "                    # predict noise\n",
    "                    noise_pred = ema_noise_pred_net(\n",
    "                        sample=naction,\n",
    "                        timestep=k,\n",
    "                        global_cond=obs_cond\n",
    "                    )\n",
    "\n",
    "                    # inverse diffusion step (remove noise)\n",
    "                    naction = noise_scheduler.step(\n",
    "                        model_output=noise_pred,\n",
    "                        timestep=k,\n",
    "                        sample=naction\n",
    "                    ).prev_sample\n",
    "\n",
    "            # unnormalize action\n",
    "            naction = naction.detach().to('cpu').numpy()\n",
    "            # (B, pred_horizon, action_dim)\n",
    "            naction = naction[0]\n",
    "            action_pred = dproc.unnormalize_data(naction, stats=stats['action'])\n",
    "\n",
    "            # only take action_horizon number of actions\n",
    "            start = obs_horizon - 1\n",
    "            end = start + action_horizon\n",
    "            action = action_pred[start:end,:]\n",
    "            # print(action[0])\n",
    "            # print(actions_test[0])\n",
    "            traj.extend(action)\n",
    "            losses = []\n",
    "\n",
    "            # if len(action) <= len(actions_test):\n",
    "            #     lenths = len(action)\n",
    "\n",
    "            # else :\n",
    "            #     lenths = len(actions_test)\n",
    "                \n",
    "            for i in range(len(action)):\n",
    "                # loss\n",
    "                # print(\"Action_pred -\",action[i])\n",
    "                # print(\"Action_orignal -\",actions_test[i])\n",
    "                # print(\"Obs_added -\",com_obs[i])\n",
    "                if len(action) > len(actions_test):\n",
    "                    done = True\n",
    "                if done:\n",
    "                    break\n",
    "                loss_test = nn.functional.mse_loss(torch.tensor(action[i]), torch.tensor(actions_test[i]))\n",
    "                obs_deque.append(com_obs[i])\n",
    "                losses.append(loss_test.item())\n",
    "                # update progress bar\n",
    "                step_idx += 1\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix(loss=np.mean(losses))\n",
    "                # print(i)\n",
    "                if step_idx > max_steps:\n",
    "                    done = True\n",
    "                if done:\n",
    "                    break\n",
    "            com_obs = com_obs[len(action):]\n",
    "            actions_test = actions_test[len(action):]\n",
    "            loss_com.append(np.mean(losses).tolist())\n",
    "    losses_per_traj[f\"{name[j]}\"] = np.nanmean(loss_com)\n",
    "    trajectories[f\"{name[j]}\"] = traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "with PdfPages('/home/cam/Documents/diffusion_policy_cam/diffusion_pipline/pred_incorrect_method_velocity/pred_trajectory_chisel_plots.pdf') as pdf:\n",
    "    for index, key in enumerate(trajectories.keys()):\n",
    "\n",
    "        # if index == 1:\n",
    "        #     break\n",
    "\n",
    "        x_coords = [item[3] for item in trajectories[key]]\n",
    "        y_coords = [item[4] for item in trajectories[key]]\n",
    "        z_coords = [item[5] for item in trajectories[key]]\n",
    "        # on_off = [item[12] for item in trajectories[key]]\n",
    "\n",
    "        fig = plt.figure(figsize=(18, 24))\n",
    "        \n",
    "        # Add a global title for the figure\n",
    "        # fig.suptitle(f'Comprehensive Plots for {result_dict[index]['Path'].split('/')[-1]}', fontsize=16, fontweight='bold')\n",
    "        fig.suptitle(f'Comprehensive Plots for {key} - loss = {losses_per_traj[key]}', fontsize=16, fontweight='bold')\n",
    "\n",
    "        plt.subplots_adjust(top=0.85)\n",
    "\n",
    "        # Plot XYZ coordinates\n",
    "        plt.subplot(4, 2, 1)\n",
    "        plt.plot(range(len(x_coords)), x_coords, label='X', linestyle='-', linewidth=1)  # Adjust line width here\n",
    "        plt.plot(range(len(y_coords)), y_coords, label='Y', linestyle='-', linewidth=1)  # Adjust line width here\n",
    "        plt.plot(range(len(z_coords)), z_coords, label='Z', linestyle='-', linewidth=1)  # Adjust line width here\n",
    "        # plt.plot(range(len(on_off)), on_off, label='On-OFF', linestyle='-', linewidth=1)  # Adjust line width here\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.title(f'Pred Gripper- Euler vel')\n",
    "        plt.xlabel('Frame')\n",
    "        plt.ylabel('Velocities')\n",
    "\n",
    "        # Plot Euler angles\n",
    "        roll = [item[0] for item in trajectories[key]]\n",
    "        pitch = [item[1] for item in trajectories[key]]\n",
    "        yaw = [item[2] for item in trajectories[key]]\n",
    "\n",
    "        plt.subplot(4, 2, 2)\n",
    "        plt.plot(range(len(roll)), roll, label='Roll', linestyle='-', linewidth=1)  # Adjust line width here\n",
    "        plt.plot(range(len(pitch)), pitch, label='Pitch', linestyle='-', linewidth=1)  # Adjust line width here\n",
    "        plt.plot(range(len(yaw)), yaw, label='Yaw', linestyle='-', linewidth=1)  # Adjust line width here\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.title(f'Pred Gripper- - XYZ  vel')\n",
    "        plt.xlabel('Frame')\n",
    "        plt.ylabel('Velocities (Radians)')\n",
    "\n",
    "\n",
    "        x_coords = [item[3] for item in splits_action[index]]\n",
    "        y_coords = [item[4] for item in splits_action[index]]\n",
    "        z_coords = [item[5] for item in splits_action[index]]\n",
    "        # on_off = [item[12] for item in splits_action[index]]\n",
    "\n",
    "\n",
    "        # Plot XYZ coordinates\n",
    "        plt.subplot(4, 2, 3)\n",
    "        plt.plot(range(len(x_coords)), x_coords, label='X', linestyle='-', linewidth=1)  # Adjust line width here\n",
    "        plt.plot(range(len(y_coords)), y_coords, label='Y', linestyle='-', linewidth=1)  # Adjust line width here\n",
    "        plt.plot(range(len(z_coords)), z_coords, label='Z', linestyle='-', linewidth=1)  # Adjust line width here\n",
    "        # plt.plot(range(len(on_off)), on_off, label='On-OFF', linestyle='-', linewidth=1)  # Adjust line width here\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.title(f'Orignal Gripper- Euler vel')\n",
    "        plt.xlabel('Frame')\n",
    "        plt.ylabel('Velocities')\n",
    "\n",
    "        # Plot Velocities\n",
    "        roll = [item[0] for item in splits_action[index]]\n",
    "        pitch = [item[1] for item in splits_action[index]]\n",
    "        yaw = [item[2] for item in splits_action[index]]\n",
    "\n",
    "        plt.subplot(4, 2, 4)\n",
    "        plt.plot(range(len(roll)), roll, label='Roll', linestyle='-', linewidth=1)  # Adjust line width here\n",
    "        plt.plot(range(len(pitch)), pitch, label='Pitch', linestyle='-', linewidth=1)  # Adjust line width here\n",
    "        plt.plot(range(len(yaw)), yaw, label='Yaw', linestyle='-', linewidth=1)  # Adjust line width here\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.title(f'Orignal Gripper - XYZ  vel')\n",
    "        plt.xlabel('Frame')\n",
    "        plt.ylabel('Velocities (Radians)')\n",
    "\n",
    "\n",
    "        x_coords = [item[9] for item in trajectories[key]]\n",
    "        y_coords = [item[10] for item in trajectories[key]]\n",
    "        z_coords = [item[11] for item in trajectories[key]]\n",
    "\n",
    "        # Plot Euler vel\n",
    "        plt.subplot(4, 2, 5)\n",
    "        plt.plot(range(len(x_coords)), x_coords, label='X', linestyle='-', linewidth=1)  # Adjust line width here\n",
    "        plt.plot(range(len(y_coords)), y_coords, label='Y', linestyle='-', linewidth=1)  # Adjust line width here\n",
    "        plt.plot(range(len(z_coords)), z_coords, label='Z', linestyle='-', linewidth=1)  # Adjust line width here\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.title(f'Pred Chisel- Euler vel')\n",
    "        plt.xlabel('Frame')\n",
    "        plt.ylabel('Velocities')\n",
    "\n",
    "        # Plot Velocities\n",
    "        roll = [item[6] for item in trajectories[key]]\n",
    "        pitch = [item[7] for item in trajectories[key]]\n",
    "        yaw = [item[8] for item in trajectories[key]]\n",
    "\n",
    "        plt.subplot(4, 2, 6)\n",
    "        plt.plot(range(len(roll)), roll, label='Roll', linestyle='-', linewidth=1)  # Adjust line width here\n",
    "        plt.plot(range(len(pitch)), pitch, label='Pitch', linestyle='-', linewidth=1)  # Adjust line width here\n",
    "        plt.plot(range(len(yaw)), yaw, label='Yaw', linestyle='-', linewidth=1)  # Adjust line width here\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.title(f'Pred Chisel-  XYZ  vel')\n",
    "        plt.xlabel('Frame')\n",
    "        plt.ylabel('Velocities (Radians)')\n",
    "\n",
    "\n",
    "        x_coords = [item[9] for item in splits_action[index]]\n",
    "        y_coords = [item[10] for item in splits_action[index]]\n",
    "        z_coords = [item[11] for item in splits_action[index]]\n",
    "\n",
    "        # Plot Euler vel\n",
    "        plt.subplot(4, 2, 7)\n",
    "        plt.plot(range(len(x_coords)), x_coords, label='X', linestyle='-', linewidth=1)  # Adjust line width here\n",
    "        plt.plot(range(len(y_coords)), y_coords, label='Y', linestyle='-', linewidth=1)  # Adjust line width here\n",
    "        plt.plot(range(len(z_coords)), z_coords, label='Z', linestyle='-', linewidth=1)  # Adjust line width here\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.title(f'Orignal Chisel- Euler vel')\n",
    "        plt.xlabel('Frame')\n",
    "        plt.ylabel('Velocities')\n",
    "\n",
    "        # Plot Velocities\n",
    "        roll = [item[6] for item in splits_action[index]]\n",
    "        pitch = [item[7] for item in splits_action[index]]\n",
    "        yaw = [item[8] for item in splits_action[index]]\n",
    "\n",
    "        plt.subplot(4, 2, 8)\n",
    "        plt.plot(range(len(roll)), roll, label='Roll', linestyle='-', linewidth=1)  # Adjust line width here\n",
    "        plt.plot(range(len(pitch)), pitch, label='Pitch', linestyle='-', linewidth=1)  # Adjust line width here\n",
    "        plt.plot(range(len(yaw)), yaw, label='Yaw', linestyle='-', linewidth=1)  # Adjust line width here\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.title(f'Orignal Chisel-  XYZ  vel')\n",
    "        plt.xlabel('Frame')\n",
    "        plt.ylabel('Velocities (Radians)')\n",
    "\n",
    "\n",
    "        plt.tight_layout()\n",
    "        pdf.savefig(fig)  # Save the figure with the global title\n",
    "        plt.close(fig)\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "for index, key in enumerate(trajectories_normal.keys()):\n",
    "    print(index)\n",
    "    # Define the file path or name\n",
    "    file_path = f'diffusion_pipline/pred_incorrect_method_trainging_data_no_offset/pred_{key}'\n",
    "\n",
    "    save_type='EULER'\n",
    "    # add first rows\n",
    "    _params = {\n",
    "        'QUAT': {'len':7,\n",
    "                    'dof': ['X', 'Y', 'Z', 'w', 'x', 'y', 'z']},\n",
    "        'EULER': {'len':6,\n",
    "                    'dof': ['X', 'Y', 'Z', 'x', 'y', 'z']}\n",
    "    }\n",
    "    \n",
    "    _SUP_HEADER_ROW = ([\"RigidBody\"] * len(data.rigid_bodies) * _params[save_type]['len'] + [\"Marker\"] * len(data.markers) * 3)\n",
    "    _FPS_ROW = [\"FPS\", target_fps] + [0.0]*(len(_SUP_HEADER_ROW) - 2)\n",
    "    _rb_col_names = [f\"{rb}_{axis}\" for rb in action_item for axis in _params[save_type]['dof']]\n",
    "    _obs_col_name = [f\"{rb}_{axis}\" for rb in obs_item for axis in _params[save_type]['dof']]\n",
    "    _mk_col_names = [f\"{mk}_{axis}\" for mk in marker_name for axis in ['X', 'Y', 'Z']]\n",
    "    _HEADER_ROW = _rb_col_names + _obs_col_name + _mk_col_names\n",
    "    print(len(trajectories_normal[key]))\n",
    "    print(len(splits_obs[index]))\n",
    "    min_length = min(len(trajectories_normal[key]), len(splits_obs[index][11:]))\n",
    "\n",
    "\n",
    "    # Combine up to the minimum length\n",
    "    combined_list = [np.concatenate([x, y[12:]])  for x, y in zip(np.array(trajectories_normal[key][:min_length]), np.array(splits_obs[index][:min_length]))]\n",
    "\n",
    "    \n",
    "    # Open the file in write mode\n",
    "    with open(file_path, 'w') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(_SUP_HEADER_ROW)\n",
    "        writer.writerow(_FPS_ROW)\n",
    "        writer.writerow(_HEADER_ROW)\n",
    "        writer.writerows(combined_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_mujoco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
