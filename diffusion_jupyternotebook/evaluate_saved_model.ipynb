{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af88f3b5-f005-418e-932d-99ee1f4cd44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "from diffusers.schedulers.scheduling_ddpm import DDPMScheduler\n",
    "from diffusers.training_utils import EMAModel\n",
    "from diffusers.optimization import get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "import submodules.data_filter as _df\n",
    "import diffusion_pipline.data_processing as dproc\n",
    "import diffusion_pipline.model as md\n",
    "import submodules.cleaned_file_parser as cfp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fe3b2c9-2812-4960-bec1-e2229a8fbddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = '/home1/shuklar/diff_files/checkpoints/checkpoint_3BODY_12_markers_edge_1_step_all_no_battery_epoch_199.pth' #TODO\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57d9097e-466f-45b6-806e-b1d035ebcc15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'scheduler_state_dict', 'ema_state_dict', 'len_dataloader', 'dataset_stats', 'num_epochs', 'obs_dim', 'action_dim', 'pred_horizon', 'obs_horizon', 'action_horizon', 'target_fps', 'action_item', 'obs_item', 'marker_item', 'num_diffusion_iters'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48f43f29-f97b-4c99-9405-92574f949ac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A1', 'A2', 'A3', 'A4', 'B1', 'B2', 'C1', 'C2', 'D1', 'D2', 'D3', 'D4'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint['marker_item']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a54ed51-c5ce-49f4-b7e8-2fff568b12a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 6.677608e+07\n"
     ]
    }
   ],
   "source": [
    "# Parameters corrsponding to\n",
    "save_dir = \"/home1/shuklar/diff_files/trun_table_task/pred_csvs/\" #TODO\n",
    "test_base_dir = \"/home1/shuklar/diff_files/trun_table_task/edge_test/\" #TODO\n",
    "\n",
    "num_epochs =checkpoint['num_epochs']\n",
    "obs_dim = checkpoint['obs_dim']\n",
    "action_dim = checkpoint['action_dim']\n",
    "# parameters\n",
    "pred_horizon = checkpoint['pred_horizon']\n",
    "obs_horizon = checkpoint['obs_horizon']\n",
    "action_horizon = checkpoint['action_horizon']\n",
    "target_fps = checkpoint['target_fps']\n",
    "\n",
    "action_item = checkpoint['action_item']\n",
    "obs_item = checkpoint['obs_item']\n",
    "\n",
    "\n",
    "# create network object\n",
    "noise_pred_net = md.ConditionalUnet1D(\n",
    "    input_dim=action_dim,\n",
    "    global_cond_dim=obs_dim*obs_horizon\n",
    ")\n",
    "\n",
    "# example inputs\n",
    "noised_action = torch.randn((1, pred_horizon, action_dim))\n",
    "obs = torch.zeros((1, obs_horizon, obs_dim))\n",
    "diffusion_iter = torch.zeros((1,))\n",
    "\n",
    "# the noise prediction network\n",
    "# takes noisy action, diffusion iteration and observation as input\n",
    "# predicts the noise added to action\n",
    "noise = noise_pred_net(\n",
    "    sample=noised_action,\n",
    "    timestep=diffusion_iter,\n",
    "    global_cond=obs.flatten(start_dim=1))\n",
    "\n",
    "# illustration of removing noise\n",
    "# the actual noise removal is performed by NoiseScheduler\n",
    "# and is dependent on the diffusion noise schedule\n",
    "denoised_action = noised_action - noise\n",
    "\n",
    "# for this demo, we use DDPMScheduler with 100 diffusion iterations\n",
    "num_diffusion_iters = checkpoint['num_diffusion_iters']\n",
    "noise_scheduler = DDPMScheduler(\n",
    "    num_train_timesteps=num_diffusion_iters,\n",
    "    # the choise of beta schedule has big impact on performance\n",
    "    # we found squared cosine works the best\n",
    "    beta_schedule='squaredcos_cap_v2',\n",
    "    # clip output to [-1,1] to improve stability\n",
    "    clip_sample=True,\n",
    "    # our network predicts noise (instead of denoised action)\n",
    "    prediction_type='epsilon'\n",
    ")\n",
    "\n",
    "# device transfer\n",
    "device = torch.device('cuda')\n",
    "_ = noise_pred_net.to(device)\n",
    "\n",
    "# Exponential Moving Average\n",
    "# accelerates training and improves stability\n",
    "# holds a copy of the model weights\n",
    "ema = EMAModel(\n",
    "    parameters=noise_pred_net.parameters(),\n",
    "    power=0.75)\n",
    "\n",
    "# Standard ADAM optimizer\n",
    "# Note that EMA parametesr are not optimized\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params=noise_pred_net.parameters(),\n",
    "    lr=1e-4, weight_decay=1e-6)\n",
    "\n",
    "# Cosine LR schedule with linear warmup\n",
    "lr_scheduler = get_scheduler(\n",
    "    name='cosine',\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=200,\n",
    "    num_training_steps=checkpoint['len_dataloader'] * num_epochs\n",
    ")\n",
    "\n",
    "ema_noise_pred_net = noise_pred_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ba10ede-a527-4c62-927e-02cb55eeca44",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_pred_net.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "lr_scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "ema.load_state_dict(checkpoint['ema_state_dict'])\n",
    "start_epoch = checkpoint['epoch'] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c9eadd7-f660-4122-ae3b-75a8b60f7b0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chisel', 'gripper']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c1219a7-02e3-4fbb-9cc8-1847837b609b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['battery']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4e86cc9-c657-4568-8b5f-989b6afe0a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "dict_of_df_rigid_test = {}\n",
    "dict_of_df_marker_test = {}\n",
    "name = []\n",
    "\n",
    "# for file in os.listdir(test_base_dir):\n",
    "#     name.append(file)\n",
    "for file in os.listdir(test_base_dir):\n",
    "    if file.endswith(\".csv\"):\n",
    "        name.append(file)\n",
    "        path_name = test_base_dir + file\n",
    "        data_test = cfp.DataParser.from_quat_file(file_path = path_name, target_fps=target_fps, filter=True, window_size=15, polyorder=3)\n",
    "\n",
    "        marker_data = data_test.get_marker_Txyz() #UNCOM\n",
    "        \n",
    "        # filtered_marker_data = data_test.get_marker_Txyz() #COM\n",
    "        \n",
    "        # List of keys you want to extract\n",
    "        # keys = ['A1', 'A2', 'A3', 'A4'] #COM\n",
    "        \n",
    "        # Using dictionary comprehension to get only the specified keys\n",
    "        # marker_data = {key: filtered_marker_data[key] for key in keys} #COM\n",
    "        data_state_dict = data_test.get_rigid_TxyzRxyz()\n",
    "\n",
    "        dicts = [data_state_dict, marker_data]\n",
    "        trimmed_dicts = _df.trim_lists_in_dicts(dicts)\n",
    "\n",
    "        \n",
    "        dict_of_df_rigid_test[file] = trimmed_dicts[0]\n",
    "        dict_of_df_marker_test[file] = trimmed_dicts[1]\n",
    "\n",
    "\n",
    "item_name_test = data_test.rigid_bodies\n",
    "# marker_name_test = ['A1', 'A2', 'A3', 'A4'] #COM\n",
    "marker_name_test = data_test.markers # UNCOM\n",
    "\n",
    "\n",
    "if len(dict_of_df_rigid_test) == len(dict_of_df_marker_test):\n",
    "\n",
    "    rigiddataset_test, index_test = _df.episode_combiner(dict_of_df_rigid_test, item_name_test)\n",
    "    markerdataset_test, _ = _df.episode_combiner(dict_of_df_marker_test, marker_name_test)\n",
    "\n",
    "indexes = index_test[action_item[0]]\n",
    "action = []\n",
    "obs = []\n",
    "for i in range(indexes[-1]):\n",
    "    # a = []\n",
    "    a = np.concatenate([rigiddataset_test[item][i] for item in action_item])\n",
    "    # print(a)\n",
    "\n",
    "    b = np.concatenate([rigiddataset_test[item][i] for item in action_item] + [rigiddataset_test[item][i] for item in obs_item] + [markerdataset_test[item][i] for item in marker_name_test])\n",
    "    # print(b)\n",
    "    \n",
    "    action.append(a)\n",
    "    obs.append(b)\n",
    "    \n",
    "# All demonstration episodes are concatinated in the first dimension N\n",
    "action = np.array(action, dtype=np.float64)\n",
    "obs = np.array(obs, dtype=np.float64)\n",
    "\n",
    "# Initialize lists to store segmented data\n",
    "splits_obs = []\n",
    "splits_action = []\n",
    "previous_index = 0\n",
    "\n",
    "# Iterate through episode_ends and slice action and obs accordingly\n",
    "for index in indexes:\n",
    "    splits_obs.append(obs[previous_index:index + 1])  # Include index itself in the slice\n",
    "    splits_action.append(action[previous_index:index + 1])\n",
    "    previous_index = index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4ae9b01-061b-4600-85c9-781dcd56a021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splits_obs[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d869e51-8e65-4c45-9fd2-7d7b9ea1f21a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splits_action[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fcef65d5-b0ae-475d-964c-71aa21518722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A1', 'A2', 'A3', 'A4', 'B1', 'B2', 'C1', 'C2', 'D1', 'D2', 'D3', 'D4'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marker_name_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51749005-e196-4cfd-9dc9-9d67ab880fd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "191"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splits_action[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b6f8eaa2-f973-4197-b6e6-85b99db11e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval:  98%|█████████▊| 488/500 [00:33<00:00, 14.40it/s]\n",
      "Eval:  99%|█████████▉| 496/500 [00:34<00:00, 14.40it/s]\n",
      "Eval:  99%|█████████▉| 496/500 [00:34<00:00, 14.39it/s]\n",
      "Eval:  98%|█████████▊| 488/500 [00:33<00:00, 14.40it/s]\n",
      "Eval:  98%|█████████▊| 488/500 [00:33<00:00, 14.40it/s]\n",
      "Eval:  99%|█████████▉| 496/500 [00:34<00:00, 14.40it/s]\n",
      "Eval:  98%|█████████▊| 488/500 [00:33<00:00, 14.38it/s]\n",
      "Eval:  99%|█████████▉| 496/500 [00:34<00:00, 14.40it/s]\n",
      "Eval:  98%|█████████▊| 488/500 [00:33<00:00, 14.40it/s]\n",
      "Eval:  99%|█████████▉| 496/500 [00:34<00:00, 14.39it/s]\n",
      "Eval:  98%|█████████▊| 488/500 [00:33<00:00, 14.39it/s]\n",
      "Eval:  99%|█████████▉| 496/500 [00:34<00:00, 14.40it/s]\n",
      "Eval:  99%|█████████▉| 496/500 [00:34<00:00, 14.40it/s]\n",
      "Eval:  99%|█████████▉| 496/500 [00:34<00:00, 14.38it/s]\n",
      "Eval:  99%|█████████▉| 496/500 [00:34<00:00, 14.32it/s]\n",
      "Eval:  99%|█████████▉| 496/500 [00:34<00:00, 14.32it/s]\n",
      "Eval:  98%|█████████▊| 488/500 [00:34<00:00, 14.32it/s]\n",
      "Eval:  99%|█████████▉| 496/500 [00:34<00:00, 14.32it/s]\n",
      "Eval:  99%|█████████▉| 496/500 [00:34<00:00, 14.32it/s]\n",
      "Eval:  98%|█████████▊| 488/500 [00:34<00:00, 14.31it/s]\n",
      "Eval:  99%|█████████▉| 496/500 [00:34<00:00, 14.24it/s]\n",
      "Eval:  98%|█████████▊| 488/500 [00:34<00:00, 14.27it/s]\n",
      "Eval:  99%|█████████▉| 496/500 [00:34<00:00, 14.31it/s]\n",
      "Eval:  99%|█████████▉| 496/500 [00:34<00:00, 14.31it/s]\n",
      "Eval:  99%|█████████▉| 496/500 [00:34<00:00, 14.31it/s]\n",
      "Eval:  99%|█████████▉| 496/500 [00:34<00:00, 14.31it/s]\n",
      "Eval:  98%|█████████▊| 488/500 [00:34<00:00, 14.30it/s]\n",
      "Eval:  98%|█████████▊| 488/500 [00:34<00:00, 14.31it/s]\n",
      "Eval:  98%|█████████▊| 488/500 [00:34<00:00, 14.29it/s]\n",
      "Eval:  98%|█████████▊| 488/500 [00:34<00:00, 14.31it/s]\n",
      "Eval:  99%|█████████▉| 496/500 [00:34<00:00, 14.31it/s]\n",
      "Eval:  99%|█████████▉| 496/500 [00:34<00:00, 14.32it/s]\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "trajectories = {}\n",
    "losses_per_traj = {}\n",
    "for j in range(len(indexes)):\n",
    "    # print(j)\n",
    "    # get first observation\n",
    "    com_obs = splits_obs[j]\n",
    "    obs = splits_obs[j][0]\n",
    "    actions_test = splits_action[j]\n",
    "    # max_steps = len(actions_test) # TODO -for matching total prediction horizon\n",
    "    max_steps = 500 #TODO - for 500 prdiction horizon\n",
    "    stats = checkpoint['dataset_stats']\n",
    "    # keep a queue of last 2 steps of observations\n",
    "    obs_deque = collections.deque(\n",
    "        [obs] * obs_horizon, maxlen=obs_horizon)\n",
    "\n",
    "    # save visualization and rewards\n",
    "    done = False\n",
    "    step_idx = 0\n",
    "    traj = []\n",
    "    loss_com = []\n",
    "    with tqdm(total=max_steps, desc=\"Eval\") as pbar:\n",
    "        while not done:\n",
    "            B = 1\n",
    "            # stack the last obs_horizon (2) number of observations\n",
    "            obs_seq = np.stack(obs_deque)\n",
    "            # print(\"Obs_sep -\",obs_seq)\n",
    "            # normalize observation\n",
    "            nobs = dproc.normalize_data(obs_seq, stats=stats['obs'])\n",
    "            # print(nobs)\n",
    "            # device transfer\n",
    "            nobs = torch.from_numpy(nobs).to(device, dtype=torch.float32)\n",
    "            # infer action\n",
    "            with torch.no_grad():\n",
    "                # reshape observation to (B,obs_horizon*obs_dim)\n",
    "                obs_cond = nobs.unsqueeze(0).flatten(start_dim=1)\n",
    "                # print(obs_cond.shape)\n",
    "\n",
    "                # initialize action from Guassian noise\n",
    "                noisy_action = torch.randn(\n",
    "                    (B, pred_horizon, action_dim), device=device)\n",
    "                naction = noisy_action\n",
    "\n",
    "                # init scheduler\n",
    "                noise_scheduler.set_timesteps(num_diffusion_iters)\n",
    "\n",
    "                for k in noise_scheduler.timesteps:\n",
    "                    # predict noise\n",
    "                    noise_pred = ema_noise_pred_net(\n",
    "                        sample=naction,\n",
    "                        timestep=k,\n",
    "                        global_cond=obs_cond\n",
    "                    )\n",
    "\n",
    "                    # inverse diffusion step (remove noise)\n",
    "                    naction = noise_scheduler.step(\n",
    "                        model_output=noise_pred,\n",
    "                        timestep=k,\n",
    "                        sample=naction\n",
    "                    ).prev_sample\n",
    "\n",
    "            # unnormalize action\n",
    "            naction = naction.detach().to('cpu').numpy()\n",
    "            # (B, pred_horizon, action_dim)\n",
    "            # print(len(naction[0]))\n",
    "            naction = naction[0]\n",
    "            action_pred = dproc.unnormalize_data(naction, stats=stats['action'])\n",
    "\n",
    "            # only take action_horizon number of actions\n",
    "            start = obs_horizon - 1\n",
    "            end = start + action_horizon\n",
    "            action = action_pred[start:end,:]\n",
    "            # traj.extend(action)\n",
    "            losses = []\n",
    "                \n",
    "            for i in range(len(action)):\n",
    "            # loss\n",
    "            # print(\"Action_pred -\",action[0])\n",
    "            # print(\"Action_orignal -\",actions_test[0])\n",
    "            # # print(\"Obs_added without pred-\",com_obs[i])\n",
    "                if len(action) > len(actions_test):\n",
    "                    done = True\n",
    "                if done:\n",
    "                    break\n",
    "                # loss_test = nn.functional.mse_loss(torch.tensor(action[i]), torch.tensor(actions_test[i])) # TODO - for original prediction horizon\n",
    "                action_last = list(action[i])\n",
    "                # print(\"Action_last ---\",action_last)\n",
    "                \n",
    "                if step_idx < max_steps - len(actions_test):\n",
    "                    com_obs_part = list(obs[len(action_last):])\n",
    "                    obs_deque.append(action_last + com_obs_part)\n",
    "\n",
    "                else:\n",
    "                    com_obs_part = list(com_obs[i][len(action_last):])\n",
    "                    # print(\"Obs to add\", com_obs_part)\n",
    "                    # Concatenating prediction to the obs lists\n",
    "                    com_obs[i] = action_last + com_obs_part\n",
    "                    # print(\"Obs_added with pred -\",com_obs[i])\n",
    "                    obs_deque.append(com_obs[i])\n",
    "\n",
    "\n",
    "                \n",
    "                ############################################\n",
    "                #### original pred horizon ################\n",
    "                ############################################\n",
    "                # com_obs_part = list(com_obs[i][len(action_last):])\n",
    "                # # print(\"Obs to add\", com_obs_part)\n",
    "                # # Concatenating prediction to the obs lists\n",
    "                # com_obs[i] = action_last + com_obs_part\n",
    "                # # print(\"Obs_added with pred -\",com_obs[i])\n",
    "                # obs_deque.append(com_obs[i])\n",
    "                ############################################\n",
    "\n",
    "                \n",
    "                # losses.append(loss_test.item())\n",
    "                # update progress bar\n",
    "                step_idx += 1\n",
    "                pbar.update(1)\n",
    "                # pbar.set_postfix(loss=np.mean(losses))\n",
    "                # print(i)\n",
    "                traj.append(action_last + com_obs_part)\n",
    "                if step_idx > max_steps:\n",
    "                    done = True\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            if step_idx > max_steps - len(actions_test):\n",
    "                com_obs = com_obs[len(action):]\n",
    "                actions_test = actions_test[len(action):]\n",
    "            # com_obs = com_obs[1:]\n",
    "            # actions_test = actions_test[1:]\n",
    "            # loss_com.append(np.mean(losses).tolist())\n",
    "    # losses_per_traj[f\"{name[j]}\"] = np.nanmean(loss_com)\n",
    "    trajectories[f\"{name[j]}\"] = traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9d86f912-ef46-462b-a201-32f179ddd7c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trajectories['ft_450_edge_1_step_1.csv'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "70120e15-fa0c-4553-a9cc-c364b9cb09c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "save_dir = \"/home1/shuklar/diff_files/trun_table_task/pred_csvs/\"\n",
    "for index, key in enumerate(trajectories.keys()):\n",
    "    print(index)\n",
    "    # Define the file path or name\n",
    "    file_path = f'{save_dir}/pred_{key}'\n",
    "\n",
    "    save_type='EULER'\n",
    "    # add first rows\n",
    "    _params = {\n",
    "        'QUAT': {'len':7,\n",
    "                    'dof': ['X', 'Y', 'Z', 'w', 'x', 'y', 'z']},\n",
    "        'EULER': {'len':6,\n",
    "                    'dof': ['X', 'Y', 'Z', 'x', 'y', 'z']},\n",
    "        'Vel': {'len':6,\n",
    "                    'dof': ['Xv', 'Yv', 'Zv', 'xv', 'yv', 'zv']}\n",
    "    }\n",
    "    \n",
    "    _SUP_HEADER_ROW = ([\"RigidBody\"] *  len(data_test.rigid_bodies) * _params[save_type]['len'] + [\"Marker\"] * len(marker_name_test) * 3)\n",
    "    _FPS_ROW = [\"FPS\", target_fps] + [0.0]*(len(_SUP_HEADER_ROW) - 2)\n",
    "    _rb_col_names = [f\"{rb}_{axis}\" for rb in action_item for axis in _params[save_type]['dof']]\n",
    "    _obs_col_name = [f\"{rb}_{axis}\" for rb in obs_item for axis in _params[save_type]['dof']]\n",
    "    _mk_col_names = [f\"{mk}_{axis}\" for mk in marker_name_test for axis in ['X', 'Y', 'Z']]\n",
    "    _HEADER_ROW = _rb_col_names + _obs_col_name + _mk_col_names\n",
    "        # Open the file in write mode\n",
    "    with open(file_path, 'w') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(_SUP_HEADER_ROW)\n",
    "        writer.writerow(_FPS_ROW)\n",
    "        writer.writerow(_HEADER_ROW)\n",
    "        writer.writerows(trajectories[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de2f600-c691-4980-9931-5ef88d60d393",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.2 (default)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
